{
  "_from": "kafka-node",
  "_id": "kafka-node@4.1.3",
  "_inBundle": false,
  "_integrity": "sha512-C2WHksRCr7vIKmbxYaCk2c5Q1lnHIi6C0f3AioK3ARcRHGO9DpqErcoaS9d8PP62yzTnkYras+iAlmPsZHNSfw==",
  "_location": "/kafka-node",
  "_phantomChildren": {
    "lodash": "4.17.15"
  },
  "_requested": {
    "escapedName": "kafka-node",
    "fetchSpec": "latest",
    "name": "kafka-node",
    "raw": "kafka-node",
    "rawSpec": "",
    "registry": true,
    "saveSpec": null,
    "type": "tag"
  },
  "_requiredBy": [
    "#USER",
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/kafka-node/-/kafka-node-4.1.3.tgz",
  "_shasum": "9e5fdf3a5562370ff0f82c5481bd14a6e39ad93d",
  "_shrinkwrap": null,
  "_spec": "kafka-node",
  "_where": "/Users/junlanlu/Documents/CMPE 273/grubHub-prototype-app/Grubhub/restaurant_owner/Backend",
  "bugs": {
    "url": "https://github.com/SOHU-co/kafka-node/issues"
  },
  "bundleDependencies": false,
  "dependencies": {
    "async": "^2.6.2",
    "binary": "~0.3.0",
    "bl": "^2.2.0",
    "buffer-crc32": "~0.2.5",
    "buffermaker": "~1.2.0",
    "debug": "^2.1.3",
    "denque": "^1.3.0",
    "lodash": "^4.17.4",
    "minimatch": "^3.0.2",
    "nested-error-stacks": "^2.0.0",
    "optional": "^0.1.3",
    "retry": "^0.10.1",
    "snappy": "^6.0.1",
    "uuid": "^3.0.0"
  },
  "deprecated": false,
  "description": "Client for Apache Kafka v0.9.x, v0.10.x and v0.11.x",
  "devDependencies": {
    "@types/node": "^10.12.27",
    "coveralls": "^2.11.12",
    "doctoc": "^1.2.0",
    "eslint": "^5.14.1",
    "eslint-config-semistandard": "^13.0.0",
    "eslint-config-standard": "^12.0.0",
    "eslint-plugin-dependencies": "^2.2.0",
    "eslint-plugin-import": "^2.16.0",
    "eslint-plugin-node": "^8.0.1",
    "eslint-plugin-promise": "^4.0.1",
    "eslint-plugin-standard": "^4.0.0",
    "execa": "^0.6.1",
    "istanbul": "^0.4.4",
    "mocha": "^3.1.0",
    "optimist": "^0.6.1",
    "proxyquire": "^1.7.10",
    "should": "^6.0.0",
    "sinon": "^2.0.0",
    "through2": "^2.0.3",
    "tslint": "^5.13.0",
    "tslint-config-semistandard": "^7.0.0",
    "typescript": "^2.8.3"
  },
  "engines": {
    "node": ">=6.4.0"
  },
  "files": [
    "kafka.js",
    "lib",
    "logging.js",
    "types"
  ],
  "homepage": "https://github.com/SOHU-Co/kafka-node#readme",
  "keywords": [
    "broker",
    "consumer",
    "kafka",
    "producer"
  ],
  "license": "MIT",
  "main": "kafka.js",
  "name": "kafka-node",
  "optionalDependencies": {
    "snappy": "^6.0.1"
  },
  "readme": "Kafka-node\n==========\n\n[![Build Status](https://travis-ci.org/SOHU-Co/kafka-node.svg?branch=master)](https://travis-ci.org/SOHU-Co/kafka-node)\n[![Coverage Status](https://coveralls.io/repos/github/SOHU-Co/kafka-node/badge.svg?branch=master)](https://coveralls.io/github/SOHU-Co/kafka-node?branch=master)\n\n[![NPM](https://nodei.co/npm/kafka-node.png)](https://nodei.co/npm/kafka-node/)\n<!--[![NPM](https://nodei.co/npm-dl/kafka-node.png?height=3)](https://nodei.co/npm/kafka-node/)-->\n\n\nKafka-node is a Node.js client for Apache Kafka 0.9 and later.\n\n# Table of Contents\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n\n- [Features](#features)\n- [Install Kafka](#install-kafka)\n- [API](#api)\n  - [KafkaClient](#kafkaclient)\n  - [Producer](#producer)\n  - [HighLevelProducer](#highlevelproducer)\n  - [ProducerStream](#producerstream)\n  - [Consumer](#consumer)\n  - [ConsumerStream](#consumerstream)\n  - [ConsumerGroup](#consumergroup)\n  - [ConsumerGroupStream](#consumergroupstream)\n  - [Offset](#offset)\n  - [Admin](#admin)\n- [Troubleshooting / FAQ](#troubleshooting--faq)\n  - [HighLevelProducer with KeyedPartitioner errors on first send](#highlevelproducer-with-keyedpartitioner-errors-on-first-send)\n  - [How do I debug an issue?](#how-do-i-debug-an-issue)\n  - [For a new consumer how do I start consuming from the latest message in a partition?](#for-a-new-consumer-how-do-i-start-consuming-from-the-latest-message-in-a-partition)\n  - [ConsumerGroup does not consume on all partitions](#consumergroup-does-not-consume-on-all-partitions)\n  - [How to throttle messages / control the concurrency of processing messages](#how-to-throttle-messages--control-the-concurrency-of-processing-messages)\n  - [How do I produce and consume binary data?](#how-do-i-produce-and-consume-binary-data)\n  - [What are these node-gyp and snappy errors?](#what-are-these-node-gyp-and-snappy-errors)\n  - [How do I configure the log output?](#how-do-i-configure-the-log-output)\n  - [Error: Not a message set. Magic byte is 2](#error-not-a-message-set-magic-byte-is-2)\n- [Running Tests](#running-tests)\n- [LICENSE - \"MIT\"](#license---mit)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n# Features\n* Consumer\n* Producer and High Level Producer\n* Node Stream Producer (Kafka 0.9+)\n* Node Stream Consumers (ConsumerGroupStream Kafka 0.9+)\n* Manage topic Offsets\n* SSL connections to brokers (Kafka 0.9+)\n* SASL/PLAIN Authentication (Kafka 0.10+)\n* Consumer Groups managed by Kafka coordinator (Kafka 0.9+)\n* Connect directly to brokers (Kafka 0.9+)\n* Administrative APIs\n\t* List Groups\n\t* Describe Groups\n\t* Create Topics\n\n# Install Kafka\nFollow the [instructions](http://kafka.apache.org/documentation.html#quickstart) on the Kafka wiki to build Kafka and get a test broker up and running.\n\n# API\n\n## KafkaClient\n\nNew KafkaClient connects directly to Kafka brokers.\n\n### Options\n* `kafkaHost` : A string of kafka broker/host combination delimited by comma for example: `kafka-1.us-east-1.myapp.com:9093,kafka-2.us-east-1.myapp.com:9093,kafka-3.us-east-1.myapp.com:9093` default: `localhost:9092`.\n* `connectTimeout` : in ms it takes to wait for a successful connection before moving to the next host default: `10000`\n* `requestTimeout` : in ms for a kafka request to timeout default: `30000`\n* `autoConnect` : automatically connect when KafkaClient is instantiated otherwise you need to manually call `connect` default: `true`\n* `connectRetryOptions` : object hash that applies to the initial connection. see [retry](https://www.npmjs.com/package/retry) module for these options.\n* `idleConnection` : allows the broker to disconnect an idle connection from a client (otherwise the clients continues to O after being disconnected). The value is elapsed time in ms without any data written to the TCP socket. default: 5 minutes\n* `reconnectOnIdle` : when the connection is closed due to client idling, client will attempt to auto-reconnect. default: true\n* `maxAsyncRequests` : maximum async operations at a time toward the kafka cluster. default: 10\n* `sslOptions`: **Object**, options to be passed to the tls broker sockets, ex. `{ rejectUnauthorized: false }` (Kafka 0.9+)\n* `sasl`: **Object**, SASL authentication configuration (only SASL/PLAIN is currently supported), ex. `{ mechanism: 'plain', username: 'foo', password: 'bar' }` (Kafka 0.10+)\n\n### Example\n\n```javascript\nconst client = new kafka.KafkaClient({kafkaHost: '10.3.100.196:9092'});\n```\n\n## Producer\n### Producer(KafkaClient, [options], [customPartitioner])\n* `client`: client which keeps a connection with the Kafka server.\n* `options`: options for producer,\n\n```js\n{\n    // Configuration for when to consider a message as acknowledged, default 1\n    requireAcks: 1,\n    // The amount of time in milliseconds to wait for all acks before considered, default 100ms\n    ackTimeoutMs: 100,\n    // Partitioner type (default = 0, random = 1, cyclic = 2, keyed = 3, custom = 4), default 0\n    partitionerType: 2\n}\n```\n\n``` js\nvar kafka = require('kafka-node'),\n    Producer = kafka.Producer,\n    client = new kafka.KafkaClient(),\n    producer = new Producer(client);\n```\n\n### Events\n\n- `ready`: this event is emitted when producer is ready to send messages.\n- `error`: this is the error event propagates from internal client, producer should always listen it.\n\n### send(payloads, cb)\n* `payloads`: **Array**,array of `ProduceRequest`, `ProduceRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName',\n   messages: ['message body'], // multi messages should be a array, single message can be just a string or a KeyedMessage instance\n   key: 'theKey', // string or buffer, only needed when using keyed partitioner\n   partition: 0, // default 0\n   attributes: 2, // default: 0\n   timestamp: Date.now() // <-- defaults to Date.now() (only available with kafka v0.10+)\n}\n```\n\n* `cb`: **Function**, the callback\n\n`attributes` controls compression of the message set. It supports the following values:\n\n* `0`: No compression\n* `1`: Compress using GZip\n* `2`: Compress using snappy\n\nExample:\n\n```js\nvar kafka = require('kafka-node'),\n    Producer = kafka.Producer,\n    KeyedMessage = kafka.KeyedMessage,\n    client = new kafka.KafkaClient(),\n    producer = new Producer(client),\n    km = new KeyedMessage('key', 'message'),\n    payloads = [\n        { topic: 'topic1', messages: 'hi', partition: 0 },\n        { topic: 'topic2', messages: ['hello', 'world', km] }\n    ];\nproducer.on('ready', function () {\n    producer.send(payloads, function (err, data) {\n        console.log(data);\n    });\n});\n\nproducer.on('error', function (err) {})\n```\n\n### createTopics(topics, cb)\nThis method is used to create topics on the Kafka server. It requires Kafka 0.10+.\n\n* `topics`: **Array**, array of topics\n* `cb`: **Function**, the callback\n\nExample:\n\n``` js\nvar kafka = require('kafka-node');\nvar client = new kafka.KafkaClient();\n\nvar topicsToCreate = [{\n  topic: 'topic1',\n  partitions: 1,\n  replicationFactor: 2\n},\n{\n  topic: 'topic2',\n  partitions: 5,\n  replicationFactor: 3,\n  // Optional set of config entries\n  configEntries: [\n    {\n      name: 'compression.type',\n      value: 'gzip'\n    },\n    {\n      name: 'min.compaction.lag.ms',\n      value: '50'\n    }\n  ],\n  // Optional explicit partition / replica assignment\n  // When this property exists, partitions and replicationFactor properties are ignored\n  replicaAssignment: [\n    {\n      partition: 0,\n      replicas: [3, 4]\n    },\n    {\n      partition: 1,\n      replicas: [2, 1]\n    }\n  ]\n}];\n\nclient.createTopics(topicsToCreate, (error, result) => {\n  // result is an array of any errors if a given topic could not be created\n});\n\n```\n\n## HighLevelProducer\n### HighLevelProducer(KafkaClient, [options], [customPartitioner])\n* `client`: client which keeps a connection with the Kafka server. Round-robins produce requests to the available topic partitions\n* `options`: options for producer,\n\n```js\n{\n    // Configuration for when to consider a message as acknowledged, default 1\n    requireAcks: 1,\n    // The amount of time in milliseconds to wait for all acks before considered, default 100ms\n    ackTimeoutMs: 100\n}\n```\n\n``` js\nvar kafka = require('kafka-node'),\n    HighLevelProducer = kafka.HighLevelProducer,\n    client = new kafka.KafkaClient(),\n    producer = new HighLevelProducer(client);\n```\n\n### Events\n\n- `ready`: this event is emitted when producer is ready to send messages.\n- `error`: this is the error event propagates from internal client, producer should always listen it.\n\n### send(payloads, cb)\n* `payloads`: **Array**,array of `ProduceRequest`, `ProduceRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName',\n   messages: ['message body'], // multi messages should be a array, single message can be just a string,\n   key: 'theKey', // string or buffer, only needed when using keyed partitioner\n   attributes: 1,\n   timestamp: Date.now() // <-- defaults to Date.now() (only available with kafka v0.10 and KafkaClient only)\n}\n```\n\n* `cb`: **Function**, the callback\n\nExample:\n\n``` js\nvar kafka = require('kafka-node'),\n    HighLevelProducer = kafka.HighLevelProducer,\n    client = new kafka.KafkaClient(),\n    producer = new HighLevelProducer(client),\n    payloads = [\n        { topic: 'topic1', messages: 'hi' },\n        { topic: 'topic2', messages: ['hello', 'world'] }\n    ];\nproducer.on('ready', function () {\n    producer.send(payloads, function (err, data) {\n        console.log(data);\n    });\n});\n```\n\n### createTopics(topics, async, cb)\nThis method is used to create topics on the Kafka server. It only work when `auto.create.topics.enable`, on the Kafka server, is set to true. Our client simply sends a metadata request to the server which will auto create topics. When `async` is set to false, this method does not return until all topics are created, otherwise it returns immediately.\n\n* `topics`: **Array**,array of topics\n* `async`: **Boolean**,async or sync\n* `cb`: **Function**,the callback\n\nExample:\n\n``` js\nvar kafka = require('kafka-node'),\n    HighLevelProducer = kafka.HighLevelProducer,\n    client = new kafka.KafkaClient(),\n    producer = new HighLevelProducer(client);\n// Create topics sync\nproducer.createTopics(['t','t1'], false, function (err, data) {\n    console.log(data);\n});\n// Create topics async\nproducer.createTopics(['t'], true, function (err, data) {});\nproducer.createTopics(['t'], function (err, data) {});// Simply omit 2nd arg\n```\n\n## ProducerStream\n\n### ProducerStream (options)\n\n#### Options\n* `highWaterMark` size of write buffer (Default: 100)\n* `kafkaClient` options see [KafkaClient](#kafkaclient)\n* `producer` options for Producer see [HighLevelProducer](#highlevelproducer)\n\n### Streams Example\n\nIn this example we demonstrate how to stream a source of data (from `stdin`) to kafka (`ExampleTopic` topic) for processing. Then in a separate instance (or worker process) we consume from that kafka topic and use a `Transform` stream to update the data and stream the result to a different topic using a `ProducerStream`.\n\n> Stream text from `stdin` and write that into a Kafka Topic\n\n```js\nconst Transform = require('stream').Transform;\nconst ProducerStream = require('./lib/producerStream');\nconst _ = require('lodash');\nconst producer = new ProducerStream();\n\nconst stdinTransform = new Transform({\n  objectMode: true,\n  decodeStrings: true,\n  transform (text, encoding, callback) {\n    text = _.trim(text);\n    console.log(`pushing message ${text} to ExampleTopic`);\n    callback(null, {\n      topic: 'ExampleTopic',\n      messages: text\n    });\n  }\n});\n\nprocess.stdin.setEncoding('utf8');\nprocess.stdin.pipe(stdinTransform).pipe(producer);\n```\n\n> Use `ConsumerGroupStream` to read from this topic and transform the data and feed the result of into the `RebalanceTopic` Topic.\n\n```js\nconst ProducerStream = require('./lib/producerStream');\nconst ConsumerGroupStream = require('./lib/consumerGroupStream');\nconst resultProducer = new ProducerStream();\n\nconst consumerOptions = {\n  kafkaHost: '127.0.0.1:9092',\n  groupId: 'ExampleTestGroup',\n  sessionTimeout: 15000,\n  protocol: ['roundrobin'],\n  asyncPush: false,\n  id: 'consumer1',\n  fromOffset: 'latest'\n};\n\nconst consumerGroup = new ConsumerGroupStream(consumerOptions, 'ExampleTopic');\n\nconst messageTransform = new Transform({\n  objectMode: true,\n  decodeStrings: true,\n  transform (message, encoding, callback) {\n    console.log(`Received message ${message.value} transforming input`);\n    callback(null, {\n      topic: 'RebalanceTopic',\n      messages: `You have been (${message.value}) made an example of`\n    });\n  }\n});\n\nconsumerGroup.pipe(messageTransform).pipe(resultProducer);\n```\n\n## Consumer\n### Consumer(client, payloads, options)\n* `client`: client which keeps a connection with the Kafka server. **Note**: it's recommend that create new client for different consumers.\n* `payloads`: **Array**,array of `FetchRequest`, `FetchRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName',\n   offset: 0, //default 0\n   partition: 0 // default 0\n}\n```\n\n* `options`: options for consumer,\n\n```js\n{\n    groupId: 'kafka-node-group',//consumer group id, default `kafka-node-group`\n    // Auto commit config\n    autoCommit: true,\n    autoCommitIntervalMs: 5000,\n    // The max wait time is the maximum amount of time in milliseconds to block waiting if insufficient data is available at the time the request is issued, default 100ms\n    fetchMaxWaitMs: 100,\n    // This is the minimum number of bytes of messages that must be available to give a response, default 1 byte\n    fetchMinBytes: 1,\n    // The maximum bytes to include in the message set for this partition. This helps bound the size of the response.\n    fetchMaxBytes: 1024 * 1024,\n    // If set true, consumer will fetch message from the given offset in the payloads\n    fromOffset: false,\n    // If set to 'buffer', values will be returned as raw buffer objects.\n    encoding: 'utf8',\n    keyEncoding: 'utf8'\n}\n```\nExample:\n\n``` js\nvar kafka = require('kafka-node'),\n    Consumer = kafka.Consumer,\n    client = new kafka.KafkaClient(),\n    consumer = new Consumer(\n        client,\n        [\n            { topic: 't', partition: 0 }, { topic: 't1', partition: 1 }\n        ],\n        {\n            autoCommit: false\n        }\n    );\n```\n\n### on('message', onMessage);\nBy default, we will consume messages from the last committed offset of the current group\n\n* `onMessage`: **Function**, callback when new message comes\n\nExample:\n\n``` js\nconsumer.on('message', function (message) {\n    console.log(message);\n});\n```\n\n### on('error', function (err) {})\n\n\n### on('offsetOutOfRange', function (err) {})\n\n\n### addTopics(topics, cb, fromOffset)\nAdd topics to current consumer, if any topic to be added not exists, return error\n* `topics`: **Array**, array of topics to add\n* `cb`: **Function**,the callback\n* `fromOffset`: **Boolean**, if true, the consumer will fetch message from the specified offset, otherwise it will fetch message from the last commited offset of the topic.\n\nExample:\n\n``` js\nconsumer.addTopics(['t1', 't2'], function (err, added) {\n});\n\nor\n\nconsumer.addTopics([{ topic: 't1', offset: 10 }], function (err, added) {\n}, true);\n```\n\n### removeTopics(topics, cb)\n* `topics`: **Array**, array of topics to remove\n* `cb`: **Function**, the callback\n\nExample:\n\n``` js\nconsumer.removeTopics(['t1', 't2'], function (err, removed) {\n});\n```\n\n### commit(cb)\nCommit offset of the current topics manually, this method should be called when a consumer leaves\n\n* `cb`: **Function**, the callback\n\nExample:\n\n``` js\nconsumer.commit(function(err, data) {\n});\n```\n\n### setOffset(topic, partition, offset)\nSet offset of the given topic\n\n* `topic`: **String**\n\n* `partition`: **Number**\n\n* `offset`: **Number**\n\nExample:\n\n``` js\nconsumer.setOffset('topic', 0, 0);\n```\n\n### pause()\nPause the consumer. ***Calling `pause` does not automatically stop messages from being emitted.*** This is because pause just stops the kafka consumer fetch loop. Each iteration of the fetch loop can obtain a batch of messages (limited by `fetchMaxBytes`).\n\n### resume()\nResume the consumer. Resumes the fetch loop.\n\n### pauseTopics(topics)\nPause specify topics\n\n```\nconsumer.pauseTopics([\n    'topic1',\n    { topic: 'topic2', partition: 0 }\n]);\n```\n\n### resumeTopics(topics)\nResume specify topics\n\n```\nconsumer.resumeTopics([\n    'topic1',\n    { topic: 'topic2', partition: 0 }\n]);\n```\n\n### close(force, cb)\n* `force`: **Boolean**, if set to true, it forces the consumer to commit the current offset before closing, default `false`\n\nExample\n\n```js\nconsumer.close(true, cb);\nconsumer.close(cb); //force is disabled\n```\n\n## ConsumerStream\n\n`Consumer` implemented using node's `Readable` stream interface. Read more about streams [here](https://nodejs.org/dist/v8.1.3/docs/api/stream.html#stream_readable_streams).\n\n### Notes\n\n* streams are consumed in chunks and in `kafka-node` each chunk is a kafka message\n* a stream contains an internal buffer of messages fetched from kafka. By default the buffer size is `100` messages and can be changed through the `highWaterMark` option\n\n### Compared to Consumer\n\nSimilar API as `Consumer` with some exceptions. Methods like `pause` and `resume` in `ConsumerStream` respects the toggling of flow mode in a Stream. In `Consumer` calling `pause()` just paused the fetch cycle and will continue to emit `message` events. Pausing in a `ConsumerStream` should immediately stop emitting `data` events.\n\n### ConsumerStream(client, payloads, options)\n\n## ConsumerGroup\n\n### ConsumerGroup(options, topics)\n\n```js\nvar options = {\n  kafkaHost: 'broker:9092', // connect directly to kafka broker (instantiates a KafkaClient)\n  batch: undefined, // put client batch settings if you need them\n  ssl: true, // optional (defaults to false) or tls options hash\n  groupId: 'ExampleTestGroup',\n  sessionTimeout: 15000,\n  // An array of partition assignment protocols ordered by preference.\n  // 'roundrobin' or 'range' string for built ins (see below to pass in custom assignment protocol)\n  protocol: ['roundrobin'],\n  encoding: 'utf8', // default is utf8, use 'buffer' for binary data\n\n  // Offsets to use for new groups other options could be 'earliest' or 'none' (none will emit an error if no offsets were saved)\n  // equivalent to Java client's auto.offset.reset\n  fromOffset: 'latest', // default\n  commitOffsetsOnFirstJoin: true, // on the very first time this consumer group subscribes to a topic, record the offset returned in fromOffset (latest/earliest)\n  // how to recover from OutOfRangeOffset error (where save offset is past server retention) accepts same value as fromOffset\n  outOfRangeOffset: 'earliest', // default\n  // Callback to allow consumers with autoCommit false a chance to commit before a rebalance finishes\n  // isAlreadyMember will be false on the first connection, and true on rebalances triggered after that\n  onRebalance: (isAlreadyMember, callback) => { callback(); } // or null\n};\n\nvar consumerGroup = new ConsumerGroup(options, ['RebalanceTopic', 'RebalanceTest']);\n\n// Or for a single topic pass in a string\n\nvar consumerGroup = new ConsumerGroup(options, 'RebalanceTopic');\n```\n\n\n### Custom Partition Assignment Protocol\n\nYou can pass a custom assignment strategy to the `protocol` array with the interface:\n\n#### string :: name\n#### integer :: version\n#### object :: userData\n#### function :: assign (topicPartition, groupMembers, callback)\n**topicPartition**\n\n```json\n{\n  \"RebalanceTopic\": [\n    \"0\",\n    \"1\",\n    \"2\"\n  ],\n  \"RebalanceTest\": [\n    \"0\",\n    \"1\",\n    \"2\"\n  ]\n}\n```\n\n**groupMembers**\n\n```json\n[\n  {\n    \"subscription\": [\n      \"RebalanceTopic\",\n      \"RebalanceTest\"\n    ],\n    \"version\": 0,\n    \"id\": \"consumer1-8db1b117-61c6-4f91-867d-20ccd1ad8b3d\"\n  },\n  {\n    \"subscription\": [\n      \"RebalanceTopic\",\n      \"RebalanceTest\"\n    ],\n    \"version\": 0,\n    \"id\": \"consumer3-bf2d11f4-1c73-4a39-b498-cfe76eb65bea\"\n  },\n  {\n    \"subscription\": [\n      \"RebalanceTopic\",\n      \"RebalanceTest\"\n    ],\n    \"version\": 0,\n    \"id\": \"consumer2-9781058e-fad4-40e8-a69c-69afbae05184\"\n  }\n]\n```\n\n**callback(error, result)**\n\n***result***\n\n```json\n[\n  {\n    \"memberId\": \"consumer3-bf2d11f4-1c73-4a39-b498-cfe76eb65bea\",\n    \"topicPartitions\": {\n      \"RebalanceTopic\": [\n        \"2\"\n      ],\n      \"RebalanceTest\": [\n        \"2\"\n      ]\n    },\n    \"version\": 0\n  },\n  {\n    \"memberId\": \"consumer2-9781058e-fad4-40e8-a69c-69afbae05184\",\n    \"topicPartitions\": {\n      \"RebalanceTopic\": [\n        \"1\"\n      ],\n      \"RebalanceTest\": [\n        \"1\"\n      ]\n    },\n    \"version\": 0\n  },\n  {\n    \"memberId\": \"consumer1-8db1b117-61c6-4f91-867d-20ccd1ad8b3d\",\n    \"topicPartitions\": {\n      \"RebalanceTopic\": [\n        \"0\"\n      ],\n      \"RebalanceTest\": [\n        \"0\"\n      ]\n    },\n    \"version\": 0\n  }\n]\n```\n\n### on('message', onMessage);\nBy default, we will consume messages from the last committed offset of the current group\n\n* `onMessage`: **Function**, callback when new message comes\n\nExample:\n\n``` js\nconsumer.on('message', function (message) {\n    console.log(message);\n});\n```\n\n### on('error', function (err) {})\n\n### on('offsetOutOfRange', function (err) {})\n\n### commit(force, cb)\nCommit offset of the current topics manually, this method should be called when a consumer leaves\n\n* `force`: **Boolean**, force a commit even if there's a pending commit, default false (optional)\n* `cb`: **Function**, the callback\n\nExample:\n\n``` js\nconsumer.commit(function(err, data) {\n});\n```\n\n### pause()\nPause the consumer. ***Calling `pause` does not automatically stop messages from being emitted.*** This is because pause just stops the kafka consumer fetch loop. Each iteration of the fetch loop can obtain a batch of messages (limited by `fetchMaxBytes`).\n\n### resume()\nResume the consumer. Resumes the fetch loop.\n\n### close(force, cb)\n* `force`: **Boolean**, if set to true, it forces the consumer to commit the current offset before closing, default `false`\n\nExample:\n\n```js\nconsumer.close(true, cb);\nconsumer.close(cb); //force is disabled\n```\n\n## ConsumerGroupStream\n\nThe `ConsumerGroup` wrapped with a `Readable` stream interface. Read more about consuming `Readable` streams [here](https://nodejs.org/dist/v8.1.3/docs/api/stream.html#stream_readable_streams).\n\nSame notes in the Notes section of [ConsumerStream](#consumerstream) applies to this stream.\n\n### Auto Commit\n\n`ConsumerGroupStream` manages auto commits differently than `ConsumerGroup`. Whereas the `ConsumerGroup` would automatically commit offsets of fetched messages the `ConsumerGroupStream` will only commit offsets of consumed messages from the stream buffer. This will be better for most users since it more accurately represents what was actually \"Consumed\". The interval at which auto commit fires off is still controlled by the `autoCommitIntervalMs` option and this feature can be disabled by setting `autoCommit` to `false`.\n\n### ConsumerGroupStream (consumerGroupOptions, topics)\n\n* `consumerGroupOptions` same options to initialize a `ConsumerGroup`\n* `topics` a single or array of topics to subscribe to\n\n### commit(message, force, callback)\nThis method can be used to commit manually when `autoCommit` is set to `false`.\n\n* `message` the original message or an object with `{topic, partition, offset}`\n* `force` a commit even if there's a pending commit\n* `callback` (*optional*)\n\n### close(callback)\nCloses the `ConsumerGroup`. Calls `callback` when complete. If `autoCommit` is enabled calling close will also commit offsets consumed from the buffer.\n\n## Offset\n### Offset(client)\n* `client`: client which keeps a connection with the Kafka server.\n\n### events\n* `ready`: when all brokers are discovered\n* `connect` when broker is ready\n\n### fetch(payloads, cb)\nFetch the available offset of a specific topic-partition\n\n* `payloads`: **Array**,array of `OffsetRequest`, `OffsetRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName',\n   partition: 0, //default 0\n   // time:\n   // Used to ask for all messages before a certain time (ms), default Date.now(),\n   // Specify -1 to receive the latest offsets and -2 to receive the earliest available offset.\n   time: Date.now(),\n   maxNum: 1 //default 1\n}\n```\n\n* `cb`: *Function*, the callback\n\nExample\n\n```js\nvar kafka = require('kafka-node'),\n    client = new kafka.KafkaClient(),\n    offset = new kafka.Offset(client);\n    offset.fetch([\n        { topic: 't', partition: 0, time: Date.now(), maxNum: 1 }\n    ], function (err, data) {\n        // data\n        // { 't': { '0': [999] } }\n    });\n```\n\n### fetchCommits(groupid, payloads, cb)\n\nFetch the last committed offset in a topic of a specific consumer group\n\n* `groupId`: consumer group\n* `payloads`: **Array**,array of `OffsetFetchRequest`, `OffsetFetchRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName',\n   partition: 0 //default 0\n}\n```\n\nExample\n\n```js\nvar kafka = require('kafka-node'),\n    client = new kafka.KafkaClient(),\n    offset = new kafka.Offset(client);\n    offset.fetchCommitsV1('groupId', [\n        { topic: 't', partition: 0 }\n    ], function (err, data) {\n    });\n```\n\n### fetchCommitsV1(groupid, payloads, cb)\n\nAlias of `fetchCommits`.\n\n### fetchLatestOffsets(topics, cb)\n\nExample\n\n```js\n\tvar partition = 0;\n\tvar topic = 't';\n\toffset.fetchLatestOffsets([topic], function (error, offsets) {\n\t\tif (error)\n\t\t\treturn handleError(error);\n\t\tconsole.log(offsets[topic][partition]);\n\t});\n```\n\n### fetchEarliestOffsets(topics, cb)\n\nExample\n\n```js\n\tvar partition = 0;\n\tvar topic = 't';\n\toffset.fetchEarliestOffsets([topic], function (error, offsets) {\n\t\tif (error)\n\t\t\treturn handleError(error);\n\t\tconsole.log(offsets[topic][partition]);\n\t});\n```\n\n## Admin\n\nThis class provides administrative APIs can be used to monitor and administer the Kafka cluster.\n\n### Admin (KafkaClient)\n* `kafkaClient`: client which keeps a connection with the Kafka server.\n\n### listGroups(cb)\n\nList the consumer groups managed by the kafka cluster.\n\n* `cb`: **Function**, the callback\n\nExample:\n\n```js\nconst client = new kafka.KafkaClient();\nconst admin = new kafka.Admin(client); // client must be KafkaClient\nadmin.listGroups((err, res) => {\n  console.log('consumerGroups', res);\n});\n```\n\nResult:\n\n```js\nconsumerGroups { 'console-consumer-87148': 'consumer',\n  'console-consumer-2690': 'consumer',\n  'console-consumer-7439': 'consumer'\n}\n```\n\n### describeGroups(consumerGroups, cb)\n\nFetch consumer group information from the cluster. See result for detailed information.\n\n* `consumerGroups`: **Array**, array of consumer groups (which can be gathered from `listGroups`)\n* `cb`: **Function**, the callback\n\nExample:\n\n```js\nadmin.describeGroups(['console-consumer-2690'], (err, res) => {\n  console.log(JSON.stringify(res,null,1));\n})\n```\n\nResult:\n\n```json\n{\n \"console-consumer-2690\": {\n  \"members\": [\n   {\n    \"memberId\": \"consumer-1-20195e12-cb3b-4ba4-9076-e7da8ed0d57a\",\n    \"clientId\": \"consumer-1\",\n    \"clientHost\": \"/192.168.61.1\",\n    \"memberMetadata\": {\n     \"subscription\": [\n      \"twice-tt\"\n     ],\n     \"version\": 0,\n     \"userData\": \"JSON parse error\",\n     \"id\": \"consumer-1-20195e12-cb3b-4ba4-9076-e7da8ed0d57a\"\n    },\n    \"memberAssignment\": {\n     \"partitions\": {\n      \"twice-tt\": [\n       0,\n       1\n      ]\n     },\n     \"version\": 0,\n     \"userData\": \"JSON Parse error\"\n    }\n   }\n  ],\n  \"error\": null,\n  \"groupId\": \"console-consumer-2690\",\n  \"state\": \"Stable\",\n  \"protocolType\": \"consumer\",\n  \"protocol\": \"range\",\n  \"brokerId\": \"4\"\n }\n}\n```\n\n### listTopics(cb)\n\nList the topics managed by the kafka cluster.\n\n* `cb`: **Function**, the callback\n\nExample:\n\n```js\nconst client = new kafka.KafkaClient();\nconst admin = new kafka.Admin(client);\nadmin.listTopics((err, res) => {\n  console.log('topics', res);\n});\n```\n\nResult:\n\n```js\n[\n  {\n    \"1001\": {\n      \"nodeId\": 1001,\n      \"host\": \"127.0.0.1\",\n      \"port\": 9092\n    }\n  },\n  {\n    \"metadata\": {\n      \"my-test-topic\": {\n        \"0\": {\n          \"topic\": \"my-test-topic\",\n          \"partition\": 0,\n          \"leader\": 1001,\n          \"replicas\": [\n            1001\n          ],\n          \"isr\": [\n            1001\n          ]\n        },\n        \"1\": {\n          \"topic\": \"my-test-topic\",\n          \"partition\": 1,\n          \"leader\": 1001,\n          \"replicas\": [\n            1001\n          ],\n          \"isr\": [\n            1001\n          ]\n        }\n      }\n    },\n    \"clusterMetadata\": {\n      \"controllerId\": 1001\n    }\n  }\n]\n```\n\n### createTopics(topics, cb)\n\n```js\nvar topics = [{\n  topic: 'topic1',\n  partitions: 1,\n  replicationFactor: 2\n}];\nadmin.createTopics(topics, (err, res) => {\n  // result is an array of any errors if a given topic could not be created\n})\n```\n\nSee [createTopics](#createtopicstopics-cb)\n\n### describeConfigs(payload, cb)\n\nFetch the configuration for the specified resources. It requires Kafka 0.11+.\n\n* `payload`: **Array**, array of resources\n* `cb`: **Function**, the callback\n\nExample:\n\n```js\nconst resource = {\n  resourceType: admin.RESOURCE_TYPES.topic,   // 'broker' or 'topic'\n  resourceName: 'my-topic-name',\n  configNames: []           // specific config names, or empty array to return all,\n}\n\nconst payload = {\n  resources: [resource],\n  includeSynonyms: false   // requires kafka 2.0+\n};\n\nadmin.describeConfigs(payload, (err, res) => {\n  console.log(JSON.stringify(res,null,1));\n})\n```\n\nResult:\n\n```json\n[\n {\n  \"configEntries\": [\n   {\n    \"synonyms\": [],\n    \"configName\": \"compression.type\",\n    \"configValue\": \"producer\",\n    \"readOnly\": false,\n    \"configSource\": 5,\n    \"isSensitive\": false\n   },\n   {\n    \"synonyms\": [],\n    \"configName\": \"message.format.version\",\n    \"configValue\": \"0.10.2-IV0\",\n    \"readOnly\": false,\n    \"configSource\": 4,\n    \"isSensitive\": false\n   },\n   {\n    \"synonyms\": [],\n    \"configName\": \"file.delete.delay.ms\",\n    \"configValue\": \"60000\",\n    \"readOnly\": false,\n    \"configSource\": 5,\n    \"isSensitive\": false\n   },\n   {\n    \"synonyms\": [],\n    \"configName\": \"leader.replication.throttled.replicas\",\n    \"configValue\": \"\",\n    \"readOnly\": false,\n    \"configSource\": 5,\n    \"isSensitive\": false\n   },\n   {\n    \"synonyms\": [],\n    \"configName\": \"max.message.bytes\",\n    \"configValue\": \"1000012\",\n    \"readOnly\": false,\n    \"configSource\": 5,\n    \"isSensitive\": false\n   },\n    ...\n  ],\n  \"resourceType\": \"2\",\n  \"resourceName\": \"my-topic-name\"\n }\n]\n\n```\n\n# Troubleshooting / FAQ\n\n## HighLevelProducer with KeyedPartitioner errors on first send\n\nError:\n\n```\nBrokerNotAvailableError: Could not find the leader\n```\n\nCall `client.refreshMetadata()` before sending the first message. Reference issue [#354](https://github.com/SOHU-Co/kafka-node/issues/354)\n\n\n\n## How do I debug an issue?\nThis module uses the [debug module](https://github.com/visionmedia/debug) so you can just run below before starting your app.\n\n```bash\nexport DEBUG=kafka-node:*\n```\n\n## For a new consumer how do I start consuming from the latest message in a partition?\n\nIf you are using the new `ConsumerGroup` simply set `'latest'` to `fromOffset` option.\n\nOtherwise:\n\n1. Call `offset.fetchLatestOffsets` to get fetch the latest offset\n2. Consume from returned offset\n\nReference issue [#342](https://github.com/SOHU-Co/kafka-node/issues/342)\n\n\n## ConsumerGroup does not consume on all partitions\n\nYour partition will be stuck if the `fetchMaxBytes` is smaller than the message produced.  Increase `fetchMaxBytes` value should resolve this issue.\n\nReference to issue [#339](https://github.com/SOHU-Co/kafka-node/issues/339)\n\n## How to throttle messages / control the concurrency of processing messages\n\n1. Create a `async.queue` with message processor and concurrency of one (the message processor itself is wrapped with `setImmediate` so it will not freeze up the event loop)\n2. Set the `queue.drain` to resume the consumer\n3. The handler for consumer's `message` event pauses the consumer and pushes the message to the queue.\n\n## How do I produce and consume binary data?\n\n### Consume\nIn the consumer set the `encoding` option to `buffer`.\n\n### Produce\nSet the `messages` attribute in the `payload` to a `Buffer`. `TypedArrays` such as `Uint8Array` are not supported and need to be converted to a `Buffer`.\n\n```js\n{\n messages: Buffer.from(data.buffer)\n}\n```\n\nReference to issue [#470](https://github.com/SOHU-Co/kafka-node/issues/470) [#514](https://github.com/SOHU-Co/kafka-node/issues/514)\n\n## What are these node-gyp and snappy errors?\n\nSnappy is a optional compression library. Windows users have reported issues with installing it while running `npm install`. It's **optional** in kafka-node and can be skipped by using the `--no-optional` flag (though errors from it should not fail the install).\n\n```bash\nnpm install kafka-node --no-optional --save\n```\n\nKeep in mind if you try to use snappy without installing it `kafka-node` will throw a runtime exception.\n\n## How do I configure the log output?\n\nBy default, `kafka-node` uses [debug](https://github.com/visionmedia/debug) to log important information. To integrate `kafka-node`'s log output into an application, it is possible to set a logger provider. This enables filtering of log levels and easy redirection of output streams.\n\n### What is a logger provider?\n\nA logger provider is a function which takes the name of a logger and returns a logger implementation. For instance, the following code snippet shows how a logger provider for the global `console` object could be written:\n\n```javascript\nfunction consoleLoggerProvider (name) {\n  // do something with the name\n  return {\n    debug: console.debug.bind(console),\n    info: console.info.bind(console),\n    warn: console.warn.bind(console),\n    error: console.error.bind(console)\n  };\n}\n```\n\nThe logger interface with its `debug`, `info`, `warn` and `error` methods expects format string support as seen in `debug` or the JavaScript `console` object. Many commonly used logging implementations cover this API, e.g. bunyan, pino or winston.\n\n### How do I set a logger provider?\n\nFor performance reasons, initialization of the `kafka-node` module creates all necessary loggers. This means that custom logger providers need to be set *before requiring the `kafka-node` module*. The following example shows how this can be done:\n\n```javascript\n// first configure the logger provider\nconst kafkaLogging = require('kafka-node/logging');\nkafkaLogging.setLoggerProvider(consoleLoggerProvider);\n\n// then require kafka-node and continue as normal\nconst kafka = require('kafka-node');\n```\n\n## Error: Not a message set. Magic byte is 2\n\nIf you are receiving this error in your consumer double check the `fetchMaxBytes` configuration. If set too low the broker could start sending fetch responses in RecordBatch format instead of MessageSet.\n\n\n# Running Tests\n\n### Install Docker\n\nOn the Mac install [Docker for Mac](https://docs.docker.com/engine/installation/mac/).\n\n### Start Docker and Run Tests\n\n```bash\nnpm test\n```\n\n### Using different versions of Kafka\n\nAchieved using the `KAFKA_VERSION` environment variable.\n\n```bash\n# Runs \"latest\" kafka on docker hub*\nnpm test\n\n# Runs test against other versions:\n\nKAFKA_VERSION=0.9 npm test\n\nKAFKA_VERSION=0.10 npm test\n\nKAFKA_VERSION=0.11 npm test\n\nKAFKA_VERSION=1.0 npm test\n\nKAFKA_VERSION=1.1 npm test\n\nKAFKA_VERSION=2.0 npm test\n```\n\n*See Docker hub [tags](https://hub.docker.com/r/wurstmeister/kafka/tags/) entry for which version is considered `latest`.\n\n### Stop Docker\n\n```bash\nnpm run stopDocker\n```\n\n\n# LICENSE - \"MIT\"\nCopyright (c) 2015 Sohu.com\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/SOHU-Co/kafka-node.git"
  },
  "scripts": {
    "startDocker": "./start-docker.sh",
    "stopDocker": "docker-compose down",
    "test": "eslint . && npm run test:ts && ./run-tests.sh",
    "test:ts": "tslint --project ./types/tsconfig.json --config ./types/tslint.json && tsc --project types",
    "updateToc": "doctoc README.md --maxlevel 2 --notitle"
  },
  "types": "types/index.d.ts",
  "version": "4.1.3"
}
